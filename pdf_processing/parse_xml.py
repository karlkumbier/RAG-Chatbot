import os
import scipdf
import bs4 as bs
import re
import ntpath

from langchain.docstore.document import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import AzureOpenAIEmbeddings
from langchain.vectorstores.faiss import FAISS

UNKNOWN_AUTHORS = "Unknown authors"
UNKNOWN_YEAR = "Unknown year"
UNKNOWN_TITLE = "Unknown title"

def docs_from_pdfs(library: str):
  """" Wrapper function for parsing pdf files and generating langchain Documents from a library.

  Args:
      library (str): path to library

  Returns:
      List [Document]: list of langchain documents 
  """
  
  # Initialize papers
  papers = [d for d in os.listdir(library) if not d.startswith(".")]
  
  # Iterate over papers in lab subdirectories and parse docs
  docs = []
  for p in papers:
    try:
      pdf_file = os.path.join(library, p)
      docs.append(doc_from_xml(pdf_file))
    except:
      print(f"Error processing: {p}")

  return docs

  
def doc_from_xml(xml_file: str):
  """ Parses xml files generated by grobid and chunks each section by paragraph. Metadata for documents includes: paper title, authors, year, section, chunk.

  Args:
      xml_file (str): path to xml files as parsed by grobid.

  Returns:
      List [Document]: list of langchain documents
  """
  
  # Load xml file and parse
  with open(xml_file, 'r') as f:
    xml = f.read()
    
  xml_bs = bs.BeautifulSoup(xml)
  doc = scipdf.convert_article_soup_to_dict(xml_bs)  

  # Initialize doc metdata
  title = get_title(doc)
  authors = get_authors(doc)
  year = get_year(doc)
  ref = get_reference(authors, year, xml_file)
  
  chunked_doc = []
  
  # Parse pdf sections
  sections = []
  for s in doc.get("sections"):
    d = Document(page_content=s.get("text"))
    d.metadata["section"] = s.get("heading")
    
    if len(d.page_content):
      sections.append(d)
      

  # Split sections by paragraph
  for s in sections:
      text_splitter = RecursiveCharacterTextSplitter(
          chunk_size=1,
          separators=["\n\n", "\n"],
          chunk_overlap=0,
      )
      
      chunks = text_splitter.split_text(s.page_content)
      
      for i, chunk in enumerate(chunks):
          
          d = Document(
              page_content=re.sub("^\n*", "", chunk), 
              metadata={"section": d.metadata["section"], "chunk": i}
          )
          
          d.metadata["title"] = title
          d.metadata["reference"] = ref
          d.metadata["authors"] = authors
          d.metadata["type"] = "text"
          chunked_doc.append(d)
  
  # Parse pdf figure captions
  for f in doc.get("figures"):
    label = f.get("figure_label")
    
    if label != "":
      caption = clean_fig_caption(f.get("figure_caption"), label)
      d = Document(page_content=caption)
      d.metadata["figure"] = f"Figure {label}"
      d.metadata["title"] = title
      d.metadata["reference"] = ref
      d.metadata["authors"] = authors
      d.metadata["type"] = "figure"
      chunked_doc.append(d)
  
  return chunked_doc


def clean_fig_caption(caption: str, fig_id: int):
  """ Cleans figure caption by removing Figure X. labels"""
  caption = re.sub(f"^.*Figure {fig_id}.", "", caption) 
  caption = re.sub(f"^.*Fig {fig_id}.", "", caption)
  return re.sub("^ ", "", caption)


def get_title(doc: dict):
  """ Generates title from parsed doc. Returns unknown if title was not 
  correctly parsed."""
  title = doc.get("title")

  if title in [None, ""]:
    return UNKNOWN_TITLE
  else:
    return title

def get_year(doc: str):
  """ Generates publication year from parsed doc. If date was not correctly
  parsed, takes latest date of citations as proxy. Returns unknown if
  references and publication date were not correctly parsed."""
  year = doc.get("pub_date")
  
  if year in [None, ""]:
    ref_years = [r.get("year") for r in doc.get("references")]
    ref_years = [int(y) for y in ref_years if y.isdigit()]
    
    if len(ref_years):
      year = max(ref_years)
    else:
      year = UNKNOWN_YEAR
      
    return(str(year))
  else:
    return year.split("-")[0]

def get_authors(doc: dict):
  """ Generates author list from parsed doc. Returns unknown if authors were not correctly parsed."""
  authors = doc.get("authors")
  
  if authors in [None, ""]:
    return UNKNOWN_AUTHORS
  else:
    return authors

def get_reference(authors: str, year: str, fpath: str):
  """ Generates paper reference (author, year). Author is replaced by root file name when author names are not processed correctly."""
  
  if authors != UNKNOWN_AUTHORS:
    author_list = authors.split("; ")
    author_list = [a.split(" ")[-1] for a in author_list]
    ref_author = author_list[0]
  else:
    ref_author = ntpath.basename(fpath).replace(".grobid.tei.xml", "")
    
  return f"{ref_author} ({year})"

if __name__ == "__main__":

  phome = "/Users/kkumbier/github/persisters/"
  rhome = "/Users/kkumbier/github/RAG-Chatbot/"
  library = os.path.join(phome, "papers", "Persisters_MRD_2024-05-09", "xml")
  db = os.path.join(rhome, "faiss_db")
  
  docs = docs_from_pdfs(library)

  embedder = AzureOpenAIEmbeddings(
    deployment="text-embedding-ada-002",
  )
  
  index = FAISS.from_documents(docs, embedder)
  index.save_local(db)