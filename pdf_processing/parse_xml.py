import os
import scipdf
import bs4 as bs
import re
import ntpath
import math
import time

from langchain.docstore.document import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import AzureOpenAIEmbeddings
from langchain_community.vectorstores.faiss import FAISS

UNKNOWN_AUTHORS = "Unknown authors"
UNKNOWN_YEAR = "Unknown year"
UNKNOWN_TITLE = "Unknown title"
EMBEDDING_TPM = 300000

def docs_from_pdfs(library: str):
  """" Wrapper function for parsing pdf files and generating langchain Documents from a library.

  Args:
      library (str): path to library

  Returns:
      List [Document]: list of langchain documents 
  """
  
  # Initialize papers
  papers = [d for d in os.listdir(library) if not d.startswith(".")]
  
  # Iterate over papers in lab subdirectories and parse docs
  docs = []
  for p in papers:
    try:
      xml_file = os.path.join(library, p)
      docs += doc_from_xml(xml_file)
    except:
      print(f"Error processing: {p}")

  return docs

  
def doc_from_xml(xml_file: str):
  """ Parses xml files generated by grobid and chunks each section by paragraph. Metadata for documents includes: paper title, authors, year, section, chunk.

  Args:
      xml_file (str): path to xml files as parsed by grobid.

  Returns:
      List [Document]: list of langchain documents
  """
  
  # Load xml file and parse
  with open(xml_file, 'r') as f:
    xml = f.read()
    
  xml_bs = bs.BeautifulSoup(xml)
  doc = scipdf.convert_article_soup_to_dict(xml_bs)  

  # Initialize doc metdata
  title = get_title(doc)
  authors = get_authors(doc)
  year = get_year(doc)
  ref = get_reference(authors, year, xml_file)
  
  chunked_doc = []
  
  # Parse pdf sections
  sections = []
  for s in doc.get("sections"):
    d = Document(page_content=s.get("text"))
    d.metadata["section"] = s.get("heading")
    
    if len(d.page_content):
      sections.append(d)
      

  # Split sections by paragraph
  for s in sections:
      text_splitter = RecursiveCharacterTextSplitter(
          chunk_size=1,
          separators=["\n\n", "\n"],
          chunk_overlap=0,
      )
      
      chunks = text_splitter.split_text(s.page_content)
      
      for i, chunk in enumerate(chunks):
          
          d = Document(
              page_content=re.sub("^\n*", "", chunk), 
              metadata={"section": d.metadata["section"], "chunk": i}
          )
          
          d.metadata["title"] = title
          d.metadata["reference"] = ref
          d.metadata["authors"] = authors
          d.metadata["type"] = "text"
          chunked_doc.append(d)
  
  # Parse pdf figure captions
  for f in doc.get("figures"):
    label = f.get("figure_label")
    
    if label != "":
      caption = clean_fig_caption(f.get("figure_caption"), label)
      d = Document(page_content=caption)
      d.metadata["figure"] = f"Figure {label}"
      d.metadata["title"] = title
      d.metadata["reference"] = ref
      d.metadata["authors"] = authors
      d.metadata["type"] = "figure"
      chunked_doc.append(d)
  
  return chunked_doc


def clean_fig_caption(caption: str, fig_id: int):
  """ Cleans figure caption by removing Figure X. labels"""
  caption = re.sub(f"^.*Figure {fig_id}.", "", caption) 
  caption = re.sub(f"^.*Fig {fig_id}.", "", caption)
  return re.sub("^ ", "", caption)


def get_title(doc: dict):
  """ Generates title from parsed doc. Returns unknown if title was not 
  correctly parsed."""
  title = doc.get("title")

  if title in [None, ""]:
    return UNKNOWN_TITLE
  else:
    return title

def get_year(doc: str):
  """ Generates publication year from parsed doc. If date was not correctly
  parsed, takes latest date of citations as proxy. Returns unknown if
  references and publication date were not correctly parsed."""
  year = doc.get("pub_date")
  
  if year in [None, ""]:
    ref_years = [r.get("year") for r in doc.get("references")]
    ref_years = [y for y in ref_years if y is not None]
    ref_years = [int(y) for y in ref_years if y.isdigit()]
    
    if len(ref_years):
      year = max(ref_years)
    else:
      year = UNKNOWN_YEAR
      
    return(str(year))
  else:
    return year.split("-")[0]

def get_authors(doc: dict):
  """ Generates author list from parsed doc. Returns unknown if authors were not correctly parsed."""
  authors = doc.get("authors")
  
  if authors in [None, ""]:
    return UNKNOWN_AUTHORS
  else:
    return authors

def get_reference(authors: str, year: str, fpath: str):
  """ Generates paper reference (author, year). Author is replaced by root file name when author names are not processed correctly."""
  
  if authors != UNKNOWN_AUTHORS:
    author_list = authors.split("; ")
    author_list = [a.split(" ")[-1] for a in author_list]
    ref_author = author_list[0]
  else:
    ref_author = ntpath.basename(fpath).replace(".grobid.tei.xml", "")
    
  return f"{ref_author} ({year})"

def partition(lst, n):
  """Yield successive n-sized chunks from lst."""
  for i in range(0, len(lst), n):
    yield lst[i:i + n]
    
def estimate_tokens(docs):
  """Estimates number of tokens in doc list using 2 tokens / word"""
  tokens = [len(d.page_content.split(" ")) for d in docs]
  return 2 * sum(tokens) 
  
if __name__ == "__main__":

  phome = "/Users/kkumbier/github/persisters/"
  rhome = "/Users/kkumbier/github/RAG-Chatbot/"
  library = os.path.join(phome, "papers", "Persisters_MRD_2024-05-09", "xml")
  db = os.path.join(rhome, "faiss_db")
  
  docs = docs_from_pdfs(library)

  # Estimate number of embedding batches from token counts
  n_tokens = estimate_tokens(docs) 
  n_batch = math.ceil(n_tokens / EMBEDDING_TPM)
  batch_size = math.ceil(len(docs) / n_batch)
  
  embedder = AzureOpenAIEmbeddings(
    deployment="text-embedding-3-large-1",
  )

  index = None
  pdocs = partition(docs, batch_size) 
  
  for i, d in enumerate(pdocs):  
    print(f"Embedding doc batch {i}")
    if i == 0:
      index = FAISS.from_documents(d, embedder)
    else:
      time.sleep(60)
      index.add_documents(d)
  
  index.save_local(db)